{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "\n",
    "\n",
    "class Detokenizer(object):\n",
    "    '''\n",
    "    'Detokenize' is a stateless class that contains a single static method 'detokenize'\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def __switch(tokens, find, replace):\n",
    "        new_list = []\n",
    "\n",
    "        iterator = iter(tokens)\n",
    "        for token in iterator:\n",
    "            if token == find:\n",
    "                token = replace\n",
    "\n",
    "            new_list.append(token)\n",
    "\n",
    "        return new_list\n",
    "\n",
    "    @staticmethod\n",
    "    def __box_forward(tokens, find, replace):\n",
    "        new_list = []\n",
    "\n",
    "        iterator = iter(tokens)\n",
    "        for token in iterator:\n",
    "            if token == find:\n",
    "                token = replace + next(iterator)\n",
    "\n",
    "            new_list.append(token)\n",
    "\n",
    "        return new_list\n",
    "\n",
    "    @staticmethod\n",
    "    def __box_backwards(tokens, find, replace):\n",
    "\n",
    "        new_list = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token != find:\n",
    "                new_list.append(token)\n",
    "\n",
    "            else:\n",
    "\n",
    "                if len(new_list) > 0:\n",
    "                    new_list[-1] = new_list[-1] + replace\n",
    "                # otherwise just ignore - not much to be done ...\n",
    "\n",
    "        return new_list\n",
    "\n",
    "    @staticmethod\n",
    "    def detokenize(tokens):\n",
    "        '''\n",
    "        '''\n",
    "        new_tokens = tokens\n",
    "\n",
    "        new_tokens = Detokenizer.__box_forward(new_tokens, '-LRB-', '(')\n",
    "        new_tokens = Detokenizer.__box_forward(new_tokens, '``', '\"')\n",
    "\n",
    "        new_tokens = Detokenizer.__box_backwards(new_tokens, '-RRB-', ')')\n",
    "        new_tokens = Detokenizer.__box_backwards(new_tokens, '\\'\\'', '\"')\n",
    "\n",
    "        new_tokens = Detokenizer.__switch(new_tokens, '--', '-')\n",
    "\n",
    "        detokenizer = Detok()\n",
    "        return detokenizer.detokenize(new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('out.csv', 'w', encoding='utf-8', newline='')\n",
    "csv_out = csv.writer(output, lineterminator='\\n')\n",
    "fieldnames = ['tac_doc_id', \n",
    "              'tac_sentence_id', \n",
    "              'sentence', \n",
    "              'indexed_tokens', \n",
    "              'subj_start', \n",
    "              'subj_end', \n",
    "              'obj_start', \n",
    "              'obj_end',\n",
    "              'assessment',\n",
    "              'alt_subj_start', \n",
    "              'alt_subj_end', \n",
    "              'alt_obj_start', \n",
    "              'alt_obj_end']\n",
    "csv_out.writerow(fieldnames)\n",
    "\n",
    "\n",
    "input_stream = open(r'C:\\Users\\jyellin\\re_1\\tacred\\data\\json\\test.json', encoding='utf-8')\n",
    "json_stream = ijson.items(input_stream, 'item')\n",
    "detokenizer = Detokenizer()\n",
    "for item in filter(lambda item: item['relation'] == 'per:cities_of_residence', json_stream):\n",
    "    tokens = item['token']\n",
    "    sentence = detokenizer.detokenize(tokens)\n",
    "    indexed_tokens = [(i, token) for i, token in enumerate(tokens) ]\n",
    "    subj_start = item['subj_start']\n",
    "    subj_end = item['subj_end']\n",
    "    obj_start = item['obj_start']\n",
    "    obj_end = item['obj_end']\n",
    "    doc_id =  item['docid']\n",
    "    sentence_id = item['id']\n",
    "    assesment = None\n",
    "    alt_subj_start = None\n",
    "    alt_subj_end = None\n",
    "    alt_obj_start = None\n",
    "    alt_obj_end = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    csv_out.writerow([doc_id, \n",
    "                     sentence_id, \n",
    "                     sentence, \n",
    "                     indexed_tokens, \n",
    "                     subj_start,\n",
    "                     subj_end, \n",
    "                     obj_start,\n",
    "                     obj_end,\n",
    "                     assesment,\n",
    "                     alt_subj_start,\n",
    "                     alt_subj_end,\n",
    "                     alt_obj_start, \n",
    "                     alt_obj_end])\n",
    "                     \n",
    "                     \n",
    "                     \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
